require('dotenv').config();
const fs = require('fs');
const csv = require('csv-parser');
const { createClient } = require('@supabase/supabase-js');

// Initialize Supabase client
const supabaseUrl = process.env.SUPABASE_URL;
const supabaseKey = process.env.SUPABASE_KEY;

if (!supabaseUrl || !supabaseKey) {
    console.error("Supabase URL or Key is missing. Make sure they are set in your .env file.");
    process.exit(1);
}

const supabase = createClient(supabaseUrl, supabaseKey);

const DATASET_FILE_PATH = './malicious_phish.csv'; // Updated path to your CSV dataset
const BATCH_SIZE = 100; // Insert records in batches

async function importData() {
    const results = [];
    let currentBatch = [];

    console.log(`Starting import from ${DATASET_FILE_PATH}...`);

    fs.createReadStream(DATASET_FILE_PATH)
        .pipe(csv())
        .on('data', (data) => {
            // --- Adapt this part to match your CSV columns ---
            const url = data.url; // Matches your CSV header
            const typeFromFile = data.type ? data.type.toLowerCase() : null; // Matches your CSV header 'type'

            let status; // This will be 'safe', 'malicious', 'unknown', 'error'

            if (!url || !typeFromFile) {
                console.warn("Skipping row due to missing URL or type:", data);
                return; // Skip this row
            }

            switch (typeFromFile) {
                case 'phishing':
                case 'defacement':
                case 'malware': // Assuming 'malware' might also be a type
                    status = 'malicious';
                    break;
                case 'benign':
                    status = 'safe';
                    break;
                default:
                    console.warn(`Unknown type '${typeFromFile}' for URL '${url}'. Defaulting to 'unknown'.`);
                    status = 'unknown';
                    break;
            }
            // --- End of adaptation section ---

            if (url && status) {
                currentBatch.push({
                    url: url,
                    status: status,
                    // virustotal_analysis_id: null, // Set to null if not in CSV
                    // raw_response: { "source": "dataset", "original_type": typeFromFile }, // Optional: store original type
                    last_scanned_at: new Date(), // Use current date as last_scanned_at for imported data
                });

                if (currentBatch.length >= BATCH_SIZE) {
                    results.push(insertBatch([...currentBatch])); // Push a copy of the batch
                    currentBatch = []; // Reset batch
                }
            } else {
                console.warn("Skipping row due to missing URL or status:", data);
            }
        })
        .on('end', async () => {
            if (currentBatch.length > 0) {
                results.push(insertBatch([...currentBatch])); // Insert any remaining records
            }

            console.log('CSV file successfully processed. Waiting for all batches to complete...');
            try {
                const allBatchResults = await Promise.all(results);
                let successfulInserts = 0;
                let failedInserts = 0;
                let duplicateInserts = 0; 
                allBatchResults.forEach(batchResult => {
                    successfulInserts += batchResult.successCount;
                    failedInserts += batchResult.failureCount;
                    duplicateInserts += batchResult.duplicateCount; 
                });
                console.log(`\nImport complete!`);
                console.log(`Successfully inserted records: ${successfulInserts}`);
                console.log(`Failed to insert records (errors): ${failedInserts}`);
                console.log(`Skipped duplicate records: ${duplicateInserts}`);
            } catch (error) {
                console.error("Error processing batches:", error);
            }
        })
        .on('error', (error) => {
            console.error('Error reading CSV file:', error);
        });
}

async function insertBatch(batch) {
    if (batch.length === 0) return { successCount: 0, failureCount: 0, duplicateCount: 0 };
    console.log(`Processing batch of ${batch.length} records (inserting one by one)...`);

    let successCount = 0;
    let failureCount = 0;
    let duplicateCount = 0;

    for (const record of batch) {
        try {
            const { error } = await supabase
                .from('scanned_urls')
                .insert(record);

            if (error) {
                // Check for unique constraint violation (PostgreSQL error code 23505)
                if (error.code === '23505') {
                    // console.log(`Duplicate URL (skipped): ${record.url}`);
                    duplicateCount++;
                } else {
                    console.error(`Failed to insert URL ${record.url}:`, error.message);
                    failureCount++;
                }
            } else {
                successCount++;
            }
        } catch (e) {
            console.error(`Unexpected error inserting URL ${record.url}:`, e.message);
            failureCount++;
        }
    }
    console.log(`Batch processed. Success: ${successCount}, Failures: ${failureCount}, Duplicates: ${duplicateCount}`);
    return { successCount, failureCount, duplicateCount };
}

// Run the import
importData();

// Modify the main loop to sum up duplicateCount as well
// ... in .on('end', async () => { ...
// try {
//     const allBatchResults = await Promise.all(results);
//     let successfulInserts = 0;
//     let failedInserts = 0;
//     let duplicateInserts = 0; // Add this
//     allBatchResults.forEach(batchResult => {
//         successfulInserts += batchResult.successCount;
//         failedInserts += batchResult.failureCount;
//         duplicateInserts += batchResult.duplicateCount; // Add this
//     });
//     console.log(`\nImport complete!`);
//     console.log(`Successfully inserted records: ${successfulInserts}`);
//     console.log(`Failed to insert records (errors): ${failedInserts}`);
//     console.log(`Skipped duplicate records: ${duplicateInserts}`); // Add this
// } catch (error) {
//     console.error("Error processing batches:", error);
// }
// ...
